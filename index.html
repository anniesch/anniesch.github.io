<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/jpg" href="images/profile_old.png">
  <title>Annie S. Chen</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Annie S. Chen</name>
              </p>
              <p>Hi! I am a final-year computer science PhD student at Stanford University advised by Prof. <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> and affiliated with the <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory (SAIL)</a>. 
                <!-- My research focuses on developing robust and adaptable machine learning systems that are capable of handling distribution shifts and efficiently respond to new information. -->
                <!-- My research goal is to create robust and adaptable machine learning systems that are prepared for distribution changes and efficiently respond to new information.  -->
                <!-- The focus of my work is on pushing the boundaries of robustness and adaptability in deep learning models in supervised settings and bringing these advances to build more intelligent sequential decision-making agents, resulting in more reliable systems for deployment.
                I am particularly interested in developing robust and adaptable models that respond efficiently and effectively to new information and distribution changes.  -->
                <!-- My work spans both supervised learning and reinforcement learning settings, focusing on scalable methods that help models generalize and adapt to changing environments. -->
                <!-- I am excited by problems involving interesting distributional shifts in various settings and work to develop models that can generalize or adapt to changing environments.  -->
                I am supported by an <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a> and an <a href="https://openai.com/index/superalignment-fast-grants/">OpenAI Superalignment Fellowship</a>.  
              </p>
              <p>I recently spent six wonderful months as a full-time student researcher at Google DeepMind in London. Previously, in 2021, I received a joint B.S. in math and M.S. in computer science, both also at Stanford. I was also a research intern at <a href="https://research.google/teams/brain/">Google Brain</a>, where I learned a lot working with <a href="http://www.peteflorence.com/">Pete Florence</a>. 
              </p>
              <p>I am originally from Boulder, Colorado, and outside of research, I enjoy spending time outdoors (hiking and backpacking), playing tennis, and learning to play the guitar. 
                For three years I organized the <a href="https://stanfordcsmentoring.com/">Stanford CS Undergraduate Mentoring Program</a> to help undergraduate students get involved with computer science research. 
                <!-- I care about creating an inclusive research culture, and for three years I organized the <a href="https://stanfordcsmentoring.com/">Stanford CS Undergraduate Mentoring Program</a>, which matches undergraduate students with graduate student mentors and aims to increase the participation of underrepresented minorities in computer science research.   -->
              </p>

              <p> My research focuses on developing principled methods for advancing the capabilities and reliability of foundation models as they become widely deployed in long-tailed use cases. 
                I am excited by a broad range of machine learning topics, including <strong>reinforcement learning, data curation, test-time reasoning, robustness, and embodied AI</strong>.
              </p>

              <p>In particular, I have been interested in how we can design closed-loop pipelines that leverage a model's predictions and errors as information-rich probes of its weaknesses and opportunities for exploration. 
                Some of my recent work has shown how we can harness such online experience to push model performance through curating high-quality data [12], efficient exploration in reinforcement learning [14], and adaptive test-time reasoning at the timestep level and strategy level [10, 11]. 
                I have also worked on training and fine-tuning models to handle spurious correlations and generalize out-of-distribution [2, 5, 7, 8]. 
                <!-- and how that loop can be used to improve reliability. a model's behavior through its own predictions and errors as an information‑rich probe of a model’s latent weaknesses. and  #my early work tackles failure modes arising from known distribution shifts,-->
                <!-- I believe that reliability requires understanding a model's behavior under test-time settings and this can provide high information gain for exploration. -->
                <!-- extracting information-dense signals from the model and design closed-loop pipelines that turn experience in continual model improvements. -->
                
                <!-- My work has spanned settings from language modeling to robot learning.  -->
              </p>

              <!-- <p>
                I will be seeking full-time opportunities in industry this fall. Please reach out via email if I might be a good fit for your team or organization!
              </p> -->
              <!-- <p> My research focuses on developing robust and adaptable machine learning systems that are capable of handling distribution shifts and efficiently respond to new information.
                Here are some problems I've been interested in:
                <ol>
                <li>
                  <strong>Understanding and Manipulating Data Distributions:</strong>
                  <ul>
                    <li>
                      How does the composition and quality of training data influence robustness, and how do we manipulate the data distribution to expose models to the right information?
                    </li>
                  </ul>
                </li>
                <li>
                  <strong>On-The-Fly Adaptation:</strong>
                  <ul>
                    <li>
                      What are approaches that effectively leverage prior knowledge and foundation models to adapt behavior at test time? How do we develop good signals for steering behavior during deployment?
                    </li>
                  </ul>
                </li>
                <li>
                  <strong>Autonomous Improvement:</strong>
                  <ul>
                    <li>
                      What are effective methods to facilitate models to learn and generalize from their own behavior? How do we effectively train and fine-tune models leveraging failures, self-generated feedback, and prior experience, and how do we provide or generate useful data for this process?
                    </li>
                  </ul>
                </li>
              </ol>
              </p>
              
              <p>Please feel free to reach out about research or any advice I can help with!
              </p>
              <p  align="center">[<a href="mailto:asc8@cs.stanford.edu">Email</a>]
                [<a href="data/2025_03_CV.pdf">CV</a>]
                 [<a href="https://scholar.google.com/citations?user=bslhbWgAAAAJ&hl=en&oi=ao">Google Scholar</a>]
                 [<a href="https://twitter.com/_anniechen_">Twitter</a>]
                 [<a href="https://www.linkedin.com/in/annie-s-chen/">LinkedIn</a>]
                 [<a href="https://github.com/anniesch">GitHub</a>]
              </p> -->
            </td>
            <td width="10%" valign="top">
              <img src="images/profile.jpg"
              width="160" 
              height="240">
            </td>
          </tr>


    <table width="800" border="0" align="center" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <!-- <p> My research focuses on methods for advancing the capabilities and reliability of foundation models as they scale and become widely deployed. 
            I am excited by a broad range of machine learning topics, including robustness and adaptation to distribution shifts, reinforcement learning, and embodied AI. 
            In particular, here are some problems I've been working on recently:
            <ol>
            <li>
              <strong>Understanding and Curating Data:</strong>
              <ul>
                <li>
                  How does the composition and quality of training data influence robustness, and how do we curate data to expose models to the right information?
                </li>
              </ul>
            </li>
            <li>
              <strong>Effective Self-Improvement:</strong>
              <ul>
                <li>
                  What are effective methods to facilitate models to learn and generalize from their own behavior? How do we effectively train and fine-tune models leveraging failures, self-generated feedback, and prior experience, and how do we provide or generate useful data for this process?
                </li>
              </ul>
            </li>
            <li>
              <strong>Test-Time Reasoning and Adaptation:</strong>
              <ul>
                <li>
                  What are approaches that effectively leverage prior knowledge and foundation models to adapt behavior at test time? How do we develop good signals for steering behavior during deployment?
                </li>
              </ul>
            </li>
          </ol>
          </p> -->
          
          <!-- <p>I will be looking for full-time positions in industry this fall. Please reach out if you think I could be a good fit for your team or organization! -->
          <!-- </p> -->
          <p  align="center">[<a href="mailto:asc8@cs.stanford.edu">Email</a>]
            [<a href="data/2025_07_CV.pdf">CV</a>]
             [<a href="https://scholar.google.com/citations?user=bslhbWgAAAAJ&hl=en&oi=ao">Google Scholar</a>]
             [<a href="https://twitter.com/_anniechen_">Twitter</a>]
             [<a href="https://www.linkedin.com/in/annie-s-chen/">LinkedIn</a>]
             [<a href="https://github.com/anniesch">GitHub</a>]
          </p>
        </td>
      </tr>


        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Selected Research</heading>
              <p> Please see my CV or Google Scholar for a full list of work.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse;">

          <!-- PAPER ENTRY: DGN -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/dgn.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/pdf/2506.07505">
                <papertitle>[14] Reinforcement Learning via Implicit Imitation Guidance</papertitle>
              </a><br>
              Perry Dong*, Alec M. Lessing*, <strong>Annie S. Chen*</strong>, Chelsea Finn<br>
              <em>Under submission</em>, 2025<br>
              [<a href="https://arxiv.org/pdf/2506.07505">PDF</a>]<br>
              We introduce Data-Guided Noise (DGN), a framework that uses expert data to shape exploration in order to improve sample efficiency for online reinforcement learning.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>

          <!-- PAPER ENTRY: PIP -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/dgn.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="papers/pip.pdf">
                <papertitle>[13] Exploiting Policy Idling for Dexterous Manipulation.</papertitle>
              </a><br>
              <strong>Annie S. Chen</strong>, Philemon Brakel, Antonia Bronars, Annie Xie, Sandy Huang, Oliver Groth, Maria Bauza, Markus Wulfmeier, Nicolas Heess, Dushyant Rao.<br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2025<br>
              [<a href="papers/pip.pdf">PDF</a>]<br>
              We leverage the behavior of policy idling, where policies get stuck in a small region of states, to guide targeted exploration and iterative improvement via pause-induced perturbations.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
        
          <!-- PAPER ENTRY: Demo-SCORE -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/demoscore.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/pdf/2503.03707">
                <papertitle>[12] Curating Demonstrations with Online Experience</papertitle>
              </a><br>
              <strong>Annie S. Chen*</strong>, Alec M. Lessing*, Yuejiang Liu, Chelsea Finn<br>
              <em>Robotics: Science and Systems (RSS)</em>, 2025<br>
              [<a href="https://arxiv.org/pdf/2503.03707">PDF</a>]
              [<a href="https://anniesch.github.io/demo-score/">Website</a>]<br>
              Data curation is crucial but is usually difficult and tedious. We introduce Demo-SCORE, an automatic way to curate, informed by online experience.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
        
          <!-- PAPER ENTRY: VLM-PC -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/vlmpc.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2407.02666">
                <papertitle>[11] Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models</papertitle>
              </a><br>
              <strong>Annie S. Chen*</strong>, Alec M. Lessing*, Andy Tang*, Govind Chada*, Laura Smith, Sergey Levine, Chelsea Finn<br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2025<br>
              [<a href="https://arxiv.org/pdf/2407.02666">PDF</a>]
              [<a href="https://anniesch.github.io/vlm-pc/">Website</a>]
              [<a href="https://github.com/stanford-iris-lab/vlm-pc">Code</a>]<br>
              We propose VLM-PC to provide adaptive high-level planning, so that robots can get unstuck by exploring multiple strategies.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
        
          <!-- PAPER ENTRY: ROAM -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/roam.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2311.01059">
                <papertitle>[10] Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment</papertitle>
              </a><br>
              <strong>Annie S. Chen*</strong>, Govind Chada*, Laura Smith, Archit Sharma, Zipeng Fu, Sergey Levine, Chelsea Finn<br>
              <em>Conference on Lifelong Learning Agents (CoLLAs)</em>, 2025<br>
              [<a href="https://arxiv.org/pdf/2311.01059.pdf">PDF</a>]
              [<a href="https://anniesch.github.io/adapt-on-the-go">Website</a>]
              [<a href="https://github.com/gchada/ROAM">Code</a>]<br>
              We propose Robust Autonomous Modulation (ROAM), a framework for efficiently leveraging pre-trained behaviors to adapt at each timestep to changing situations at deployment time.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
        
          <!-- PAPER ENTRY: ATS -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/ats.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2409.19817">
                <papertitle>[9] Calibrating Language Models with Adaptive Temperature Scaling</papertitle>
              </a><br>
              Johnathan Xie*, <strong>Annie S. Chen*</strong>, Yoonho Lee, Eric Mitchell, Chelsea Finn<br>
              <em>EMNLP</em>, 2024<br>
              [<a href="https://arxiv.org/pdf/2409.19817">PDF</a>]
              [<a href="https://github.com/johnathan-xie/adaptive-temperature-scaling">Code</a>]<br>
              RLHF often degrades the calibration of pre-trained LLMs. We propose a lightweight post-hoc calibration method, Adaptive Temperature Scaling (ATS), which addresses post-RLHF calibration degradation while maintaining performance improvements.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
        
          <!-- PAPER ENTRY: COSMOS -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/cosmos_v2.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2306.11120">
                <papertitle>[8] Confidence-Based Model Selection: When to Take Shortcuts for Subpopulation Shifts</papertitle>
              </a><br>
              <strong>Annie S. Chen</strong>, Yoonho Lee, Amrith Setlur, Sergey Levine, Chelsea Finn<br>
              <em>NeurIPS DistShift Workshop</em>, 2023<br>
              [<a href="https://arxiv.org/pdf/2306.11120.pdf">PDF</a>]<br>
              We propose COSMOS, a method that adaptively selects models with different strengths to perform well on both majority and minority subpopulations without needing target labels or group annotations.            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
        
          <!-- You can continue similarly for the rest: Pro^2, Voltron, Surgical Fine-Tuning, SLRL, DVD, JTT, BEE -->
        
          <tr>
            <td width="25%" valign="top">
              <img src="images/Pro2.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2302.05441">
                <papertitle>[7] Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features</papertitle>
              </a><br>
              <strong>Annie S. Chen*</strong>, Yoonho Lee*, Amrith Setlur, Sergey Levine, Chelsea Finn<br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024 <font color="red"><strong>(Spotlight (top 5%))</strong></font><br>
              [<a href="https://arxiv.org/pdf/2302.05441.pdf">PDF</a>]<br>
              We propose Project and Probe (Pro^2), a lightweight + data-efficient approach for domain adaptation by learning diverse features. 
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
          
          <!-- PAPER ENTRY: Voltron -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/voltron.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2302.12766">
                <papertitle>[6] Language-Driven Representation Learning for Robotics</papertitle>
              </a><br>
              Siddharth Karamcheti, Suraj Nair, <strong>Annie S. Chen</strong>, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang<br>
              <em>Robotics: Science and Systems (RSS)</em>, 2023 <font color="red"><strong>(Best Paper Finalist)</strong></font><br>
              [<a href="https://arxiv.org/pdf/2302.12766.pdf">PDF</a>]
              [<a href="https://sites.google.com/view/voltron-robotics">Website</a>]
              [<a href="https://github.com/siddk/voltron-robotics">Code</a>]<br>
              We propose Voltron, which uses language to learn better visual representations for a diverse range of robotics problems by trading off conditioning and generation.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
          
          <!-- PAPER ENTRY: Surgical Fine-Tuning -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/surgical.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2210.11466">
                <papertitle>[5] Surgical Fine-Tuning Improves Adaptation to Distribution Shifts</papertitle>
              </a><br>
              Yoonho Lee*, <strong>Annie S. Chen*</strong>, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn<br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2023<br>
              [<a href="https://arxiv.org/pdf/2210.11466.pdf">PDF</a>]
              [<a href="https://github.com/anniesch/surgical-finetuning">Code</a>]<br>
              We show that selectively fine-tuning a subset of layers (surgical fine-tuning) outperforms fine-tuning all layers and reveals insights into the type of distribution shift present in the data.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
          
          <!-- PAPER ENTRY: SLRL -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/teaser_qwale.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2210.08863">
                <papertitle>[4] You Only Live Once: Single-Life Reinforcement Learning</papertitle>
              </a><br>
              <strong>Annie S. Chen</strong>, Archit Sharma, Sergey Levine, Chelsea Finn<br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2022<br>
              [<a href="https://arxiv.org/pdf/2210.08863.pdf">PDF</a>]
              [<a href="https://github.com/anniesch/single-life-rl">Code</a>]<br>
              We introduce Single-Life RL, where agents must adapt to novel tasks in a single trial without supervision, and propose QWALE, to guide agents when out-of-distribution to recover to prior experience.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
          
          <!-- PAPER ENTRY: DVD -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/pullfig_v1.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2103.16817">
                <papertitle>[3] Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos</papertitle>
              </a><br>
              <strong>Annie S. Chen</strong>, Suraj Nair, Chelsea Finn<br>
              <em>Robotics Science and Systems (RSS)</em>, 2021<br>
              [<a href="https://arxiv.org/pdf/2103.16817.pdf">PDF</a>]
              [<a href="https://sites.google.com/view/dvd-human-videos">Website</a>]
              [<a href="https://github.com/anniesch/dvd">Code</a>]<br>
              We propose DVD: reward functions learned from in-the-wild human videos that generalize to new environments and tasks.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
          
          <!-- PAPER ENTRY: JTT -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/jtt.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2107.09044">
                <papertitle>[2] Just Train Twice: Improving Group Robustness without Training Group Information</papertitle>
              </a><br>
              Evan Z. Liu*, Behzad Haghgoo*, <strong>Annie S. Chen*</strong>, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, Chelsea Finn<br>
              <em>International Conference on Machine Learning (ICML)</em>, 2021 <font color="red"><strong>(Long Talk (top 3%))</strong></font><br>
              [<a href="https://arxiv.org/pdf/2107.09044.pdf">PDF</a>]
              [<a href="https://github.com/anniesch/jtt">Code</a>]<br>
              JTT improves worst-group performance without needing group labels by extracting and upsampling difficult, informative examples.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
          
          <!-- PAPER ENTRY: BEE -->
          <tr>
            <td width="25%" valign="top">
              <img src="images/teaser_bee_4.png" width="160" style="margin: 0;">
            </td>
            <td width="75%" valign="top">
              <a href="https://arxiv.org/abs/2010.11917">
                <papertitle>[1] Batch Exploration with Examples for Scalable Robotic Reinforcement Learning</papertitle>
              </a><br>
              <strong>Annie S. Chen*</strong>, Hyunji Nam*, Suraj Nair*, Chelsea Finn<br>
              <em>Robotics and Automation Letters (RA-L)</em>, 2021<br>
              [<a href="https://arxiv.org/pdf/2010.11917.pdf">PDF</a>]
              [<a href="https://sites.google.com/view/batch-exploration">Website</a>]
              [<a href="https://github.com/stanford-iris-lab/batch-exploration">Code</a>]<br>
              BEE uses weak human supervision to guide better robotic exploration for scalable data collection, enabling better offline RL.
            </td>
          </tr>
          <tr><td colspan="2" style="height: 10px;"></td></tr>
          

        </table>
        

          <!-- <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/foundation_models.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2108.07258">
                <papertitle>On the Opportunities and Risks of Foundation Models</papertitle>
            </a>
              <br>
                Rishi Bommasani, ..., <strong>Annie S. Chen*</strong>, ..., Percy Liang
              <br>
              <em>Report by the Center for Research on Foundation Models (CRFM)</em>, 2021 
              <br>
              [<a href="https://arxiv.org/pdf/2108.07258.pdf">PDF</a>]
              <br>
            </td>
          </tr>

          <tr>
            <td width="25%">
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/1810.02425">
                <papertitle>Limit Theorems for Descents in Permutations and Arithmetic Progressions in Z/pZ</papertitle>
            </a>
              <br>
              Bryce Cai, <strong>Annie S. Chen</strong>, Ben Heller, Eyob Tsegaye
              <br>
              <em>Joint Mathematics Meetings (JMM) Undergraduate Poster Session</em>, 2019 <font color="red"><strong>(Outstanding Poster Presentation)</strong></font>
              <br>
              [<a href="https://arxiv.org/pdf/1810.02425.pdf">PDF</a>]
              <br>
            </td>
          </tr>

          <tr>
            <td width="25%">
            </td>
            <td valign="middle" width="75%">
            <a href="https://nyjm.albany.edu/j/2017/23-45v.pdf">
                <papertitle>Index Divisibility in Dynamical Sequences and Cyclic Orbits Modulo p</papertitle>
            </a>
              <br>
                <strong>Annie S. Chen</strong>, T. Alden Gassert, Katherine E. Stange
              <br>
              <em>New York Journal of Mathematics (NYJM)</em>, 2017
              <br>
              [<a href="https://nyjm.albany.edu/j/2017/23-45v.pdf">PDF</a>]
              <br>
            </td>
          </tr> -->

          

        </td>
    </tr>
  </table>

  <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
      <tbody><tr>
        <td>
          <br>
          <p align="center">
            <font size="2">
              Website template from <a href="https://github.com/jonbarron/jonbarron_website">here.</a>
            </font>
          </p>
        </td>
      </tr>
    </tbody>
  </table>

</body>

</html>
